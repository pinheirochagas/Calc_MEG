{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# Authors: Jean-Remi King <jeanremi.king@gmail.com>\n",
    "#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
    "#          Denis Engemann <denis.engemann@gmail.com>\n",
    "#\n",
    "# License: BSD (3-clause)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import GeneralizationAcrossTime\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading or reinstalling data archive MNE-sample-data-processed.tar.gz at location /volatile/anaconda/lib/python2.7/examples\n",
      "Downloading data from https://s3.amazonaws.com/mne-python/datasets/MNE-sample-data-processed.tar.gz (1.35 GB)\n",
      "\n",
      "[........................................] 100.00000 / (1.35 GB / 1.35 GB)   \n",
      "Verifying download hash.\n",
      "Decompressing the archive: /volatile/anaconda/lib/python2.7/examples/MNE-sample-data-processed.tar.gz\n",
      "(please be patient, this can take some time)\n",
      "Opening raw data file /volatile/anaconda/lib/python2.7/examples/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif...\n",
      "    Read a total of 4 projection items:\n",
      "        PCA-v1 (1 x 102)  idle\n",
      "        PCA-v2 (1 x 102)  idle\n",
      "        PCA-v3 (1 x 102)  idle\n",
      "        Average EEG reference (1 x 60)  idle\n",
      "Current compensation grade : 0\n",
      "    Range : 6450 ... 48149 =     42.956 ...   320.665 secs\n",
      "Ready.\n",
      "Reading 0 ... 41699  =      0.000 ...   277.709 secs...\n",
      "[done]\n",
      "Band-pass filtering from 1 - 30 Hz\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "data_path = sample.data_path()\n",
    "# Load and filter data, set up epochs\n",
    "raw_fname = data_path + '/MEG/sample/sample_audvis_filt-0-40_raw.fif'\n",
    "events_fname = data_path + '/MEG/sample/sample_audvis_filt-0-40_raw-eve.fif'\n",
    "raw = mne.io.Raw(raw_fname, preload=True)\n",
    "picks = mne.pick_types(raw.info, meg=True, exclude='bads')  # Pick MEG channels\n",
    "raw.filter(1, 30, method='fft')  # Band pass filtering signals\n",
    "events = mne.read_events(events_fname)\n",
    "event_id = {'AudL': 1, 'AudR': 2, 'VisL': 3, 'VisR': 4}\n",
    "decim = 2  # decimate to make the example faster to run\n",
    "epochs = mne.Epochs(raw, events, event_id, -0.050, 0.400, proj=True,\n",
    "                    picks=picks, baseline=None, preload=True,\n",
    "                    reject=dict(mag=5e-12), decim=decim, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique classes' labels are: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# We will train the classifier on all left visual vs auditory trials\n",
    "# and test on all right visual vs auditory trials.\n",
    "\n",
    "# In this case, because the test data is independent from the train data,\n",
    "# we test the classifier of each fold and average the respective predictions.\n",
    "\n",
    "# Define events of interest\n",
    "triggers = epochs.events[:, 2]\n",
    "viz_vs_auditory = np.in1d(triggers, (1, 2)).astype(int)\n",
    "\n",
    "gat = GeneralizationAcrossTime(predict_mode='mean-prediction', n_jobs=1)\n",
    "\n",
    "# For our left events, which ones are visual?\n",
    "viz_vs_auditory_l = (triggers[np.in1d(triggers, (1, 3))] == 3).astype(int)\n",
    "# To make scikit-learn happy, we converted the bool array to integers\n",
    "# in the same line. This results in an array of zeros and ones:\n",
    "print(\"The unique classes' labels are: %s\" % np.unique(viz_vs_auditory_l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 2, 3, 1, 4, 2, 3, 1, 4,\n",
       "       2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4,\n",
       "       2, 3, 1, 4, 2, 3, 1, 4, 2, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4,\n",
       "       2, 3, 1, 4, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 2,\n",
       "       3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 4, 2, 3, 1, 4, 2,\n",
       "       3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 1, 4, 2, 3, 1, 4, 2,\n",
       "       3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 3, 1, 4, 2, 3, 1, 4, 3, 1, 4, 2, 3,\n",
       "       1, 4, 2, 3, 1, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3,\n",
       "       4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 3, 1,\n",
       "       4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 2, 3, 1, 4, 2, 3, 1,\n",
       "       4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1,\n",
       "       4, 2, 3, 1, 4, 2, 3, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4, 2, 3, 1, 4,\n",
       "       2, 3, 1, 4, 2, 1, 4, 2, 3, 1, 4], dtype=uint32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gat.fit(epochs[('AudL', 'VisL')], y=viz_vs_auditory_l)\n",
    "\n",
    "# For our right events, which ones are visual?\n",
    "viz_vs_auditory_r = (triggers[np.in1d(triggers, (2, 4))] == 4).astype(int)\n",
    "\n",
    "gat.score(epochs[('AudR', 'VisR')], y=viz_vs_auditory_r)\n",
    "gat.plot(\n",
    "    title=\"Generalization Across Time (visual vs auditory): left to right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
